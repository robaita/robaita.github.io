{"componentChunkName":"component---node-modules-lekoarts-gatsby-theme-minimal-blog-core-src-templates-blog-query-tsx","path":"/blog/","result":{"data":{"allPost":{"nodes":[{"slug":"/blog/OneXOne_Convolution","title":"1x1 Convolution","date":"11.07.2024","excerpt":"Overview Recently, while diving into the Self-Attention Model applied to images, I came across the term 1x1 convolutions. At first, it…","timeToRead":3,"description":"Why does someone use a 1x1 convolution? And why is it called 1x1? It sounds like a pixel-sized joke! When it first showed up, people probably thought it was a prank. But hold on, let's dive deep into this mysterious little guy and unravel the math magic behind it.","tags":[{"name":"CNN","slug":"cnn"},{"name":"TensorFlow","slug":"tensor-flow"}]},{"slug":"/blog/attention","title":"Attention Please!","date":"22.07.2024","excerpt":"Overview This article would discuss the following concepts\n\nBasic intuation of attention model Mathematics behind the attention model Imple…","timeToRead":6,"description":"Attention in machine learning is like the brain's spotlight, highlighting important parts of data. Introduced in 2014, it focuses on relevant information using \"queries,\" \"keys,\" and \"values.\" Imagine a model yelling, \"Hey, brain, remember that detail!\" to handle complex tasks like translating long sentences.","tags":[{"name":"CNN","slug":"cnn"},{"name":"Attention","slug":"attention"},{"name":"TensorFlow","slug":"tensor-flow"}]}]}},"pageContext":{"formatString":"DD.MM.YYYY"}},"staticQueryHashes":["2421966660","2744905544","3090400250"],"slicesMap":{}}