{"componentChunkName":"component---node-modules-lekoarts-gatsby-theme-minimal-blog-core-src-templates-post-query-tsx-content-file-path-content-posts-attention-index-mdx","path":"/blog/attention/","result":{"data":{"post":{"slug":"/blog/attention","title":"Attention Please!","date":"22.07.2024","tags":[{"name":"CNN","slug":"cnn"},{"name":"Attention","slug":"attention"},{"name":"TensorFlow","slug":"tensor-flow"}],"description":"Attention in machine learning is like the brain's spotlight, highlighting important parts of data. Introduced in 2014, it focuses on relevant information using \"queries,\" \"keys,\" and \"values.\" Imagine a model yelling, \"Hey, brain, remember that detail!\" to handle complex tasks like translating long sentences.","canonicalUrl":null,"excerpt":"Overview This article would discuss the following concepts\n\nBasic intuation of attention model Mathematics behind the attention model Impleâ€¦","timeToRead":6,"banner":null}},"pageContext":{"slug":"/blog/attention","formatString":"DD.MM.YYYY","frontmatter":{"title":"Attention Please!","description":"Attention in machine learning is like the brain's spotlight, highlighting important parts of data. Introduced in 2014, it focuses on relevant information using \"queries,\" \"keys,\" and \"values.\" Imagine a model yelling, \"Hey, brain, remember that detail!\" to handle complex tasks like translating long sentences.","date":"2024-07-22T00:00:00.000Z","slug":"/blog/attention","tags":["CNN","Attention","TensorFlow"]}}},"staticQueryHashes":["2421966660","2744905544","3090400250"],"slicesMap":{}}