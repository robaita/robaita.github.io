<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Robotics and Artificial Intelligence Training Academy]]></title><description><![CDATA[Robotics and Artificial Intelligence Training Academy, Machine Learning, Computer Vision, Deep Learning, Natural Language Processing, Generative AI, Artificial Intelligence]]></description><link>https://robaita.com</link><generator>GatsbyJS</generator><lastBuildDate>Thu, 08 Aug 2024 04:42:45 GMT</lastBuildDate><item><title><![CDATA[1x1 Convolution]]></title><link>https://robaita.com/blog/OneXOne_Convolution</link><guid isPermaLink="false">https://robaita.com/blog/OneXOne_Convolution</guid><pubDate>Wed, 10 Jul 2024 18:30:00 GMT</pubDate><content:encoded>&lt;p&gt;Overview Recently, while diving into the Self-Attention Model applied to images, I came across the term 1x1 convolutions. At first, it…&lt;/p&gt;&lt;div style=&quot;margin-top: 50px; font-style: italic;&quot;&gt;&lt;strong&gt;&lt;a href=&quot;https://robaita.com/blog/OneXOne_Convolution&quot;&gt;Keep reading&lt;/a&gt;.&lt;/strong&gt;&lt;/div&gt;&lt;br /&gt; &lt;br /&gt;</content:encoded></item><item><title><![CDATA[Attention Please!]]></title><link>https://robaita.com/blog/attention</link><guid isPermaLink="false">https://robaita.com/blog/attention</guid><pubDate>Sun, 21 Jul 2024 18:30:00 GMT</pubDate><content:encoded>&lt;p&gt;Overview This article would discuss the following concepts

Basic intuation of attention model Mathematics behind the attention model Imple…&lt;/p&gt;&lt;div style=&quot;margin-top: 50px; font-style: italic;&quot;&gt;&lt;strong&gt;&lt;a href=&quot;https://robaita.com/blog/attention&quot;&gt;Keep reading&lt;/a&gt;.&lt;/strong&gt;&lt;/div&gt;&lt;br /&gt; &lt;br /&gt;</content:encoded></item></channel></rss>