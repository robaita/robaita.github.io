"use strict";(self.webpackChunkRobaita=self.webpackChunkRobaita||[]).push([[857],{1173:function(e,t,n){n.d(t,{p:function(){return p},A:function(){return m}});var a=n(6540),o=n(557),r=n(6835),i=n(3328),l=n(7715),s=n(7169);var c=e=>{let{post:t}=e;return null};const u=["16px","8px","4px"].map((e=>`rgba(0, 0, 0, 0.1) 0px ${e} ${e} 0px`));var d=e=>{let{data:{post:t},children:n}=e;return(0,o.Y)(i.A,null,(0,o.Y)(r.DZ,{as:"h1",variant:"styles.h1"},t.title),(0,o.Y)("p",{sx:{color:"secondary",mt:3,a:{color:"secondary"},fontSize:[1,1,2]}},(0,o.Y)("time",null,t.date),t.tags&&(0,o.Y)(a.Fragment,null," — ",(0,o.Y)(l.A,{tags:t.tags})),t.timeToRead&&" — ",t.timeToRead&&(0,o.Y)("span",null,t.timeToRead," min read")),(0,o.Y)("section",{sx:{my:5,".gatsby-resp-image-wrapper":{my:[4,4,5],borderRadius:"4px",boxShadow:u.join(", "),".gatsby-resp-image-image":{borderRadius:"4px"}},variant:"layout.content"}},n),(0,o.Y)(c,{post:t}))};const p=e=>{var t,n,a;let{data:{post:r}}=e;return(0,o.Y)(s.A,{title:r.title,description:r.description?r.description:r.excerpt,image:r.banner?null===(t=r.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.resize)||void 0===a?void 0:a.src:void 0,pathname:r.slug,canonicalUrl:r.canonicalUrl})};function m(e){let{...t}=e;return a.createElement(d,t)}},7715:function(e,t,n){var a=n(557),o=n(6540),r=n(4794),i=n(3601),l=n(2174);t.A=e=>{let{tags:t}=e;const{tagsPath:n,basePath:s}=(0,i.A)();return(0,a.Y)(o.Fragment,null,t.map(((e,t)=>(0,a.Y)(o.Fragment,{key:e.slug},!!t&&", ",(0,a.Y)(r.Link,{sx:e=>{var t;return{...null===(t=e.styles)||void 0===t?void 0:t.a}},to:(0,l.A)(`/${s}/${n}/${e.slug}`)},e.name)))))}},7169:function(e,t,n){var a=n(6540),o=n(4794),r=n(7533);t.A=e=>{let{title:t="",description:n="",pathname:i="",image:l="",children:s=null,canonicalUrl:c=""}=e;const u=(0,r.A)(),{siteTitle:d,siteTitleAlt:p,siteUrl:m,siteDescription:h,siteImage:f,author:g,siteLanguage:b}=u,A={title:t?`${t} | ${d}`:p,description:n||h,url:`${m}${i||""}`,image:`${m}${l||f}`};return a.createElement(a.Fragment,null,a.createElement("html",{lang:b}),a.createElement("title",null,A.title),a.createElement("meta",{name:"description",content:A.description}),a.createElement("meta",{name:"image",content:A.image}),a.createElement("meta",{property:"og:title",content:A.title}),a.createElement("meta",{property:"og:url",content:A.url}),a.createElement("meta",{property:"og:description",content:A.description}),a.createElement("meta",{property:"og:image",content:A.image}),a.createElement("meta",{property:"og:type",content:"website"}),a.createElement("meta",{property:"og:image:alt",content:A.description}),a.createElement("meta",{name:"twitter:card",content:"summary_large_image"}),a.createElement("meta",{name:"twitter:title",content:A.title}),a.createElement("meta",{name:"twitter:url",content:A.url}),a.createElement("meta",{name:"twitter:description",content:A.description}),a.createElement("meta",{name:"twitter:image",content:A.image}),a.createElement("meta",{name:"twitter:image:alt",content:A.description}),a.createElement("meta",{name:"twitter:creator",content:g}),a.createElement("meta",{name:"gatsby-theme",content:"@lekoarts/gatsby-theme-minimal-blog"}),a.createElement("link",{rel:"icon",type:"image/png",sizes:"32x32",href:(0,o.withPrefix)("/favicon-32x32.png")}),a.createElement("link",{rel:"icon",type:"image/png",sizes:"16x16",href:(0,o.withPrefix)("/favicon-16x16.png")}),a.createElement("link",{rel:"apple-touch-icon",sizes:"180x180",href:(0,o.withPrefix)("/apple-touch-icon.png")}),c?a.createElement("link",{rel:"canonical",href:c}):null,s)}},8529:function(e,t,n){n.r(t),n.d(t,{Head:function(){return l.p},default:function(){return s}});var a=n(6540),o=n(8453);function r(e){const t=Object.assign({h2:"h2",p:"p",blockquote:"blockquote",ol:"ol",li:"li",code:"code",pre:"pre",sup:"sup",a:"a",span:"span",section:"section"},(0,o.RP)(),e.components);return a.createElement(a.Fragment,null,a.createElement(t.h2,null,"Overview"),"\n",a.createElement(t.p,null,"This article would discuss the following concepts"),"\n",a.createElement(t.blockquote,null,"\n",a.createElement(t.ol,null,"\n",a.createElement(t.li,null,"Basic intuation of attention model"),"\n",a.createElement(t.li,null,"Mathematics behind the attention model"),"\n",a.createElement(t.li,null,"Implementation of attention mechanism"),"\n",a.createElement(t.li,null,"Use of Tensorflow ",a.createElement(t.code,null,"tf.keras.layers.MultiHeadAttention")," API"),"\n",a.createElement(t.li,null,"Multi-Head Attention\nWithout further delay, lets deep dive into what is attention mechanism, why it is so popular?"),"\n"),"\n"),"\n",a.createElement(t.h2,null,"Basic intuation of attention model"),"\n",a.createElement(t.p,null,"Imagine you are reading a book, and you come across a character whose actions are essential to understand the current plot. To make sense of what's happening, you need to remember previous details about this character. Here’s how the attention mechanism works in this context: ",a.createElement("br"),"\n",a.createElement("strong",null,"Query:"),' Think of the query as the question you\'re asking while reading. For example, "What has this character done before?" ',a.createElement("br"),"\n",a.createElement("strong",null,"Key:")," The key is like an index or a set of clues that help you find the right information in the book. It's the reference to different points in the story where this character is mentioned. ",a.createElement("br"),"\n",a.createElement("strong",null,"Value:")," The value is the actual information you need about the character, like their past actions and traits."),"\n",a.createElement(t.p,null,"If we explain the same example in terms of python data types then it would be something similar to as described in the below example. The query is the question that we are looking for, key is the reference that can be ",a.createElement(t.code,null,"mahabharata_characters")," and value would be the value of the query. So if we are looking for the character ",a.createElement(t.code,null,"Ashwatthama")," then the ",a.createElement(t.code,null,"mahabharata_characters")," would be the reference, treated as key and the value as ",a.createElement(t.code,null,"The formidable warrior and son of Dronacharya, known for his unrelenting pursuit of revenge against the Pandavas"),"."),"\n",a.createElement(t.pre,null,a.createElement(t.code,{className:"language-python:title=Dictionary.py"},'# dictionary of characters and their description\nmahabharata_characters = {\n    "Arjuna": "The peerless archer, whose valor and skill were unmatched on the battlefield of Kurukshetra",\n    "Krishna": "The divine charioteer and guide to Arjuna, whose wisdom and counsel in the Bhagavad Gita are legendary.",\n    "Bhishma": "The grand patriarch of the Kuru dynasty, renowned for his vow of celibacy and unparalleled warrior prowess.",\n    "Draupadi": "The fiery princess and wife of the Pandavas, whose dignity and strength became a rallying point in their quest for justice.",\n    "Karna": "The tragic hero and unparalleled warrior, known for his unwavering loyalty to Duryodhana and his inner conflict regarding his true lineage."\n    "Ashwatthama": "The formidable warrior and son of Dronacharya, known for his unrelenting pursuit of revenge against the Pandavas."\n}\n')),"\n",a.createElement(t.p,null,"Build upon this intuation, attention mechanism keeps the track of long sequences and give more weightsage to only certain portion who are useful for the token."),"\n",a.createElement(t.h2,null,"Mathematics behind the attention model"),"\n",a.createElement(t.p,null,"If we see the architecture proposed in the paper",a.createElement(t.sup,null,a.createElement(t.a,{href:"#user-content-fn-1",id:"user-content-fnref-1","data-footnote-ref":!0,"aria-describedby":"footnote-label"},"1")),", it has the following component ",a.createElement(t.code,null,"Scale"),", ",a.createElement(t.code,null,"Mask (Opt)"),", ",a.createElement(t.code,null,"Softmax")," and ",a.createElement(t.code,null,"matmul"),". These are expressed in the hand written notes.\n",a.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 562px; "\n    >\n      <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 53.75%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABJ0AAASdAHeZh94AAABwUlEQVR42n1SDW+aUBT1//+FJeuWNV1SV+3HVOpa+ylr/QBsdTqtCwgoRAt0Cigfnr33WlqT6k7yuOTdew+ce09qNBpBVVXoug5ZljEYqBgOhyQO4LouKJbLJVZhmiYGigJN02AYBuuzLIvlUkEQYD6fY7EIEIYhgmCBOI7ZXRRFWAeaN0wD0l0DjuOQ3gXrZYTJ16dTD8Ucj/zhFUShjU2I4+d6VddQujjH48ufJTwp+gijEJW7OvZy31H+WUW/J6+VugpdldAQ9zAe998T2o6N4xKHC76Ebq+D2Wz2jjB5tywbrdZvoqKGQn6fRBGd9gMUWWOjYIRjncyDu0aDK6MnNDFdQ/giGPxlE7vbV/j8oYBP5Gx/LCLzlceXrQM8/XUSyTHERoHI3cF4MiLb9TZK7nYUcLkyxFoX2W8c6pU2igUeQv2e1TNC1/VxnC8hf/QDD2R+rrtZcvPXPdLZNE7PTpE5zOLk7AS7mTRuq5W3GXqejxteQvW2hT/9zf6j0IhfRUki3lMgkEh9WxPqxEbmGyFtpP6LyLbDMHj11DrQnG3brCeJk8nkNZ9alZSc/4FukhqZ1vm+j2eFHrunhP8ATUc8bC2jp9MAAAAASUVORK5CYII=\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="attention 1"\n        title=""\n        src="/static/97fcca73755a96557680f3a08c26b760/eef63/attention-1.png"\n        srcset="/static/97fcca73755a96557680f3a08c26b760/5243c/attention-1.png 240w,\n/static/97fcca73755a96557680f3a08c26b760/ab158/attention-1.png 480w,\n/static/97fcca73755a96557680f3a08c26b760/eef63/attention-1.png 562w"\n        sizes="(max-width: 562px) 100vw, 562px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n    </span>'}}),"\nThe scaling or normalization is performed as if the value of dk become a large number, then the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by ",a.createElement(t.code,null,"1/sqrt(dk)")," ",a.createElement(t.sup,null,a.createElement(t.a,{href:"#user-content-fn-1",id:"user-content-fnref-1-2","data-footnote-ref":!0,"aria-describedby":"footnote-label"},"1")),". The mask operation is optional, this one is used, in sequence to sequence generation or similar application. When the objective of the model is to predict the next token (word), and the input feature is the collection of previous words and the next word. If we want to mask the future token, in that situation we can use this masking operation. This sets the upper tringular matrix of the dot product between query and key to infinte and when we apply softmax on top of this, the upper tringular matrix is set to zero. This we do to make the sum equal to 1. For more reference, follow this video ",a.createElement(t.sup,null,a.createElement(t.a,{href:"#user-content-fn-2",id:"user-content-fnref-2","data-footnote-ref":!0,"aria-describedby":"footnote-label"},"2")),".\n",a.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 960px; "\n    >\n      <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 139.58333333333334%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAcCAYAAABh2p9gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAD7klEQVR42m2W2ZbqOAxFnYQkkAEyQOYw3l5Vj/X/P6fWFmWK6tsPwoplH03HNq4oCpmmSeZ5lqqq5HK5mDDvnJMgCCQMQxu9+G/GKIpMZ0RclmUyDIMcj0cDRMcBep7nst1uZbfbmQO/hj3YPBi2OI4tAIfCorIsZVkW0/u+t+91XU1HxnGUw+FgQNjqujbgJEnMlqbpExAvpMvij48P07uuk/1+b2PbtjbnneCULPhm/nQ62RzpGyA/gGEk1cfjYUBsZu52u1mqzBEVa9CZp9Z+LakbIAqLCJsNb55ehfff7/ObzeZXo152PqgFUfoGUCdG6ovu7RSfaNApCWk3TfNqmNWRCABiI0J9EGrDQjaQFhl4nVJQW8rz9fVlDiiB0YYI8URhMQCEdwDYDKi3AQgYdubP61kaLRN7rperRGH0bAr18NSABkRMmqRNitWhknlVSp0aOTSV1E0t4zzKsTvKfFmk1XGbbX+67GuBZ9LGO15xgO3z81Nu603OxSxzNsi1WeVan6WNKznH2vWgkZOrpXLls4a0v6orqwvAFLc9tq9CY0vjVDK3lX2QSxWUpudBJkWwkybUwxBplC59RkhEtaZV5IVFSXTZLrNUiZjxUB1kuS5SVqV0U29pV5p+Pw0m1z83yYpcAQMn+3Iv53ySoehk0pTKai9rNkmfaYNUJ2pLv+sNvG1aOR1P8rg/ZBxGa0hZlD81LDWqcl9aA/Ist1RLoi2fYAi28/n8apbnnm/g6yx7hpMmm3wz0ButKYupracKHMUBIycLUN+8FyCEhHNwjYXX69Wi8JxkZAPACJFyE8FP9kFwvr+PoLPz2vVK4r1GMA5yf9wlTmJtRGWARMlIZNX3ERzUgQEtq8zTbCR/ASJpmEgeaS2iVJIoMdZnqQJlWletZ6up0Vm6baxQnUb0Y291fLs8nkrpcmnc3saDEjR3OyNq6yrpXGPj7DobB3dUaXVeuarfd7cqN/e/Ae0IusiAdkra1CUSu41KLFEQSRLEEgcbiXRNoYTOgq2EQWhzrI9c+Ddg4TKLgojOTjunESDMPzSKq5stwptbZHF6g+t86IL/3pc/l+avi5I5XYyE6p0oEvd8iC7bWe7pWbZJavb/BURIcXGDRbgGOoaNbDRdzu0/7iKjO1l912yWIevkmOqTUA2SpMlvQK4vpMj0dCT6JIZKmeIgdVU/GaCRpIFea3pB7MKtAU9hJxtlQqqM8O/yizacisKo8XMieGcguX/14CIjtku/yn25mW1VkvP0QnDu0l8pY+AG9lc/p8W/J9X3O7IokcdpVKDVThR3pf+X8TdtNG3/98K/dq9/BG+CnXPsH653+781R4QvI38epwAAAABJRU5ErkJggg==\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="attention 2"\n        title=""\n        src="/static/fe71b6a03016d0546283780acee4aeb2/7d769/attention-2.png"\n        srcset="/static/fe71b6a03016d0546283780acee4aeb2/5243c/attention-2.png 240w,\n/static/fe71b6a03016d0546283780acee4aeb2/ab158/attention-2.png 480w,\n/static/fe71b6a03016d0546283780acee4aeb2/7d769/attention-2.png 960w,\n/static/fe71b6a03016d0546283780acee4aeb2/9485b/attention-2.png 1155w"\n        sizes="(max-width: 960px) 100vw, 960px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n    </span>'}}),"\n",a.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 960px; "\n    >\n      <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 69.58333333333333%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAACLklEQVR42pVTXW/bMAykZPlDkh27jmM7TpMtSdOhL8MwbMDSAXvI//9PN1Ky26xvezjLEsUjKR6JshJkLCitQDrjlff5A68elPGZ3YCKls+auM/qaOd9YltkZRd9iWYYB114FN2ApGLnxCGc2Rq66ZlwzYRraA6c+DWS4YCk30Fpg8vzC67XK4ZhCGRKKZB8pm6F89Sg6SokuYPRBF+vYPoe2nOmzqKoPIaeiR0HXFXwqxVer79wu91wOBwCoVYa5NnhdH7Cy/nEkRpU7QZbjpj7CvnmETpJkegcaZrCMrE4Wi4542q+//yB1z+/cTk+4Zk+4ah2McNxHDGNA4ZNGx1sgabuYNMWqS5gVAFnHdZty1kYWLPiewkO+wO+ffmKS3nEKd1hnQZ/gjEmgNnndyA0vsGQbqHYnpJBQhoVVyOZLg1wHOis95iox9h0yG0eCd+hZhBK7TCpDSwVqMiHdfPQoXTcQMrDXiDBxO7IhqD0kWhBwiW1VC+X3qD43oJ33/ukuP2U8ZsYH/VVRJmQG1lCXELqovYEfox6nHVIbgjaVEXHMlqeQlotBtdHAQcndi5Za6JB28WAAhH9ck/WvI2iln8JHAlVPGCCIGI/xYmQSZEpEfhtJBYCCV6fQNU+VsHZBR/b3xEumUgpkmHIah6z8pGF/DkSyb+QyCoQ0hC0iwkoNTdFMiin2VCG0Yvz7WN58pbhbecMxS73pKr7eX8jNEU0SFRp0j9d+z/8BXgxNM7pR2LhAAAAAElFTkSuQmCC\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="attention 3"\n        title=""\n        src="/static/f4923867f5990a11beb3746736fcf62a/7d769/attention-3.png"\n        srcset="/static/f4923867f5990a11beb3746736fcf62a/5243c/attention-3.png 240w,\n/static/f4923867f5990a11beb3746736fcf62a/ab158/attention-3.png 480w,\n/static/f4923867f5990a11beb3746736fcf62a/7d769/attention-3.png 960w,\n/static/f4923867f5990a11beb3746736fcf62a/87339/attention-3.png 1440w,\n/static/f4923867f5990a11beb3746736fcf62a/2b633/attention-3.png 1749w"\n        sizes="(max-width: 960px) 100vw, 960px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n    </span>'}})),"\n",a.createElement(t.h2,null,"Implementation of attention mechanism"),"\n",a.createElement(t.p,null,"This section is the implementation of the above section in Tensorflow, I have taken reference from Professor Mubarak Shah Lecture, you can visit for more details ",a.createElement(t.sup,null,a.createElement(t.a,{href:"#user-content-fn-3",id:"user-content-fnref-3","data-footnote-ref":!0,"aria-describedby":"footnote-label"},"3")),"."),"\n",a.createElement(t.pre,null,a.createElement(t.code,{className:"language-python:title=attention_model.py"},'input_shape = (224, 224, 3)  # Assuming a 3-channel (RGB) image\n\n# Define the input layer\ninputs = tf.keras.Input(shape=input_shape)\n\n# Add a pre-trained ResNet50 layer (excluding the top classification layer)\nresnet50_base = applications.ResNet50(include_top=False, weights=\'imagenet\', input_tensor=inputs)\n\n# number of filters\ndq = dk = dv =3\nfeature_dimension = (resnet50_base.output_shape[1], resnet50_base.output_shape[2], dk)\nprint("Output feature dimension:",feature_dimension)\n\n# Add a 1x1 convolutional layer to get the query\nquery = layers.Conv2D(filters=dq, kernel_size=(1, 1), activation=\'relu\', padding=\'same\', name="query")(resnet50_base.output)\nprint("Query output shape:", query.shape)\n\n# Add a 1x1 convolutional layer to get the key\nkey = layers.Conv2D(filters=dk, kernel_size=(1, 1), activation=\'relu\', padding=\'same\', name="key")(resnet50_base.output)\nprint("Key output shape:", key.shape)\n\n# Add a 1x1 convolutional layer to get the value\nvalue = layers.Conv2D(filters=dv, kernel_size=(1, 1), activation=\'relu\', padding=\'same\', name="value")(resnet50_base.output)\nprint("Value output shape:", value.shape)\n\n# Flatten the conv_layer to a 2D tensor\nflattened_query = layers.Reshape((-1, 3))(query)\nflattened_key = layers.Reshape((-1, 3))(key)\nflattened_value = layers.Reshape((-1, 3))(value)\n\nprint("Flattened Query shape:", flattened_query.shape)\nprint("Flattened Key shape:", flattened_key.shape)\nprint("Flattened Value shape:", flattened_value.shape)\n\n# Transpose the flattened_key\ntransposed_key = layers.Lambda(lambda x: tf.transpose(x, perm=[0, 2, 1]))(flattened_key)\n\n# Print the shape of the transposed_key\nprint("Transposed Key shape:", transposed_key.shape)\n\n# Compute the dot product between the query and the key and scale it\nscaled_prod = layers.Dot(axes=(2,1))([flattened_query, transposed_key]) / tf.math.sqrt(tf.cast(dk, tf.float32))\nprint("Dot Product shape:", prod.shape)\n\n# Apply softmax to the attention scores\nsoftmax_layer = layers.Softmax()(scaled_prod)\nprint("Softmax shape:", softmax_layer.shape)\n\n# Compute the attention-weighted value\nattention_output = layers.Dot(axes=1)([softmax_layer, flattened_value])\nprint("Attention Output shape:", attention_output.shape)\n\nreshaped_attention_output = layers.Reshape((feature_dimension))(attention_output)\n\n\n# # Reshape the attention output to match the original image dimensions\n# reshaped_attention_output = layers.Reshape((32, 32, 3))(attention_output)\nprint("Reshaped Attention Output shape:", reshaped_attention_output.shape)\n\n# Apply a 1x1 convolutional layer to the attention output\nself_attended_feature = layers.Conv2D(filters=3, kernel_size=(1, 1), activation=\'relu\', padding=\'same\', \n                                      name="feature_map")(reshaped_attention_output)\nprint("Self-attended Feature shape:", self_attended_feature.shape)\n')),"\n",a.createElement(t.h2,null,"Use of Tensorflow ",a.createElement(t.code,null,"tf.keras.layers.MultiHeadAttention")," API"),"\n",a.createElement(t.p,null,"If you are using Tensoflow then, you may use ",a.createElement(t.code,null,"tf.keras.layers.MultiHeadAttention")," this API to use the multihead attention directly. If ",a.createElement(t.code,null,"query"),", ",a.createElement(t.code,null,"key"),", ",a.createElement(t.code,null,"value")," are the same, then this is self-attention ",a.createElement(t.sup,null,a.createElement(t.a,{href:"#user-content-fn-4",id:"user-content-fnref-4","data-footnote-ref":!0,"aria-describedby":"footnote-label"},"4")),". Each timestep in query attends to the corresponding sequence in ",a.createElement(t.code,null,"key"),", and returns a fixed-width vector."),"\n",a.createElement(t.p,null,"This layer first projects ",a.createElement(t.code,null,"query"),", ",a.createElement(t.code,null,"key")," and ",a.createElement(t.code,null,"value"),". These are (effectively) a list of tensors of length ",a.createElement(t.code,null,"num_attention_heads"),", where the corresponding shapes are (",a.createElement(t.code,null,"batch_size"),", ",a.createElement(t.code,null,"<query dimensions>"),", ",a.createElement(t.code,null,"key_dim"),"), (",a.createElement(t.code,null,"batch_size"),", ",a.createElement(t.code,null,"<key/value dimensions>"),", ",a.createElement(t.code,null,"key_dim"),"), (",a.createElement(t.code,null,"batch_size"),", ",a.createElement(t.code,null,"<key/value dimensions>"),", ",a.createElement(t.code,null,"value_dim"),")."),"\n",a.createElement(t.p,null,"Then, the query and key tensors are dot-producted and scaled. These are softmaxed to obtain attention probabilities. The value tensors are then interpolated by these probabilities, then concatenated back to a single tensor. Finally, the result tensor with the last dimension as value_dim can take a linear projection and return."),"\n",a.createElement(t.pre,null,a.createElement(t.code,{className:"language-python:title=multihead_attention_model.py"},"self_attended_feature = layers.MultiHeadAttention(num_heads=1, key_dim=3, value_dim = 3)(resnet50_base.output, resnet50_base.output)\n")),"\n",a.createElement(t.p,null,"The comparison between both the models can be seen in the below figure.\n",a.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 960px; "\n    >\n      <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 40.416666666666664%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABJ0AAASdAHeZh94AAABR0lEQVR42pVR2XKDMBDj/7+yySRgfK5PrgRVkGmax9YzOxjYlbRSNxoL5yMkFWiJcMkj54JSC6w36PseSius24q/nG5QI5QrmNoEHRMkRzTe2zTBOA2lBqScCLi9h/Z9fz9/7m9AYxxCLKh1orqMWCLvDfM8wzuHr9uFBA21VYzawgt72OuLnFtMJHbBYSCxpIBOc2UbMlJpsFQYqLCUSoBGKywbFRuFNiRcew1DwEBinwWtVuSaoYzC7X6DRAIaSw+lnkw2H+zpXHleqJDMR+OphN+M0QjFkyBiXVc8n09srMfjcdbxzpUNtM9UNMMxjETGw7+ZA048LvcrCtc9AJZlOcNZCfjkNjQXlA6i/Xo4jgzFBkT+tDFydYKzuXLIMd1xeIWyfYSCI4ijqOisj2A6EWGygjQTpJRXclS405+dXlIa/nO+AdSnavfg7sY3AAAAAElFTkSuQmCC\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="attention 4"\n        title=""\n        src="/static/0df7fd6cb7a86333f53b0b10f85f6633/7d769/attention-4.png"\n        srcset="/static/0df7fd6cb7a86333f53b0b10f85f6633/5243c/attention-4.png 240w,\n/static/0df7fd6cb7a86333f53b0b10f85f6633/ab158/attention-4.png 480w,\n/static/0df7fd6cb7a86333f53b0b10f85f6633/7d769/attention-4.png 960w,\n/static/0df7fd6cb7a86333f53b0b10f85f6633/87339/attention-4.png 1440w,\n/static/0df7fd6cb7a86333f53b0b10f85f6633/faabe/attention-4.png 1509w"\n        sizes="(max-width: 960px) 100vw, 960px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n    </span>'}})),"\n",a.createElement(t.h2,null,"Multi Head Attention"),"\n",a.createElement(t.p,null,"When the number of head is one and the query, key and value dimensions are same then same network becomes self attention. But when we use multiple heads then the same network (key, query and value) gets multiplied with the number of heads and produce those many feature maps. Let's take the same example and extend it for the multiple heads.\n",a.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 904px; "\n    >\n      <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 160.41666666666669%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAgCAYAAAASYli2AAAACXBIWXMAABJ0AAASdAHeZh94AAAEfElEQVR42pVW2ZLbNhAkwPuQeFOkSGm18h7ybioVJ///bZ2eAWV55XXFeUARBIFGz6DRQ8/zPLhmXDMWt7H/0Yxx7Tpgogi2rOF3A/y2h1+38HeTG6sa2LyA37T63aY5TJwogIli2M0WlvNNGK6A8iFJYbLcfSzYCOKXFd9LPmsd08240OYbfZo4dWSShP34ytYB+m23gm1gCawLpV+4MVlkt5Vu6jedRmHT7LPwXUd2NUmmoVgy01AEjKFKiMHyAL/fudD7UZnfcv8JoADIzoa7ClMTOUaaP4YcDKPO+Y0DWjvWurAIKMm1WaG5E1bXg9K81p1uJAwlFfJtPd07wLvm8paSYe3AFHxUUNlMI5FDZAqM9T8BDAIE0+J2HffuKfIRqdSNspUNgvmIYLfXtKi0GEH4cNb1P4V8FbXu6PsMPXCSoEaNLOC4HhJD1jD5bjhP194uiHGnSfoqCbKx21L77r3T79ofJgTjrEoI5dSZDjl9DZ06XLXoOepywpRNsD9oGAGTHRBAQg/GiYCZA5hmp88NpVWUmhLbc97AdJXtGnIYscXuxoj62bdytSTUlEnPSqcCfjPplvN8BLGPuCTzMEOaBNg2BQJq2TMEDDOGUBRcFCGMfCSV6JATA4tdl2LTbRHFGc5ziflQEajFcahwmjbI2wbHacL7uUfV1/A2BPsyd9gvNaKiwsNQ42kpUXYN9tMe719OOB07dFTA6/kJl8cR8+MZl8s7Xh9m7I8THh6f8XI+Ylmoyb7v8fXyFU+HHXbLjJeXr/jjcka/6zDOB1xeXwg4oh0GHJYDJrKoyWqeZ4xDh+1mA8FoqgrbLUPO8xzHAycOLQqGPZH+0LYo+TFiDtM0RSzWxhwmImaRDvvXpjK7eaHk2iKkxmTQ8KLLu/bXd6dNGTcIfN4ge3ejDAHv77IINdgv6JIJZ2/GuY5xGiz6bESWHeBVbygZ4vubh7Y1KysPjc9DY6v9u7sc0JnjP//BrjjhL+8Fb12O19nDfnNAkSzwum8oD4/4+5uHfrgBzqHFK+UzBnc3RcQZvb4T8IGAz3iuc7wvBrt8hzSmqOs3JB0P7MnDsAJSb1gI+EyZCbD9kaFYUHR5Q5uOePIOOFYx5s6gSjokYUNtHVW0+73HQ7rl6xD6Cnji0/+QQ5qpn20QUthbr0DBSXnsITIpIp+3IygQBQmabQjfriEroMUbQz7eM9TTNSKHUB06aCaE/YKw43O3IJlPCBvafzt9ZyeAkruFTRjaXxms2hNNQOuKmKe4Dp9iZ/bmeQo4BMwz2+Ge4YdiLZNpFldD/fgj8EPfSEoMJrLL7X+VAPFIus1HwJ9bTtCOYMJUNJn++Ocg9qSlVICklEoRyl0REssXj7wartZl8U/WloRmnNLZQ88xvpXR0B2IOrdUP3lqGYhcCZASKndXcikOLSYbuyr5sQRcAaUA0d61QJXUHjcQplqout4BSBltutWxt/dAn+RQC9Ftkv49COv1d0RSoH2y8tvB1eSfQT/5LfuNXzcNmwVt/eP63v4FVqm9XVRLVNkAAAAASUVORK5CYII=\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="attention 5"\n        title=""\n        src="/static/b929cb787bd7bd2e4cba485ca31f1a71/799c6/attention-5.png"\n        srcset="/static/b929cb787bd7bd2e4cba485ca31f1a71/5243c/attention-5.png 240w,\n/static/b929cb787bd7bd2e4cba485ca31f1a71/ab158/attention-5.png 480w,\n/static/b929cb787bd7bd2e4cba485ca31f1a71/799c6/attention-5.png 904w"\n        sizes="(max-width: 904px) 100vw, 904px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n    </span>'}})),"\n",a.createElement(t.h2,null,"References"),"\n",a.createElement(t.section,{"data-footnotes":!0,className:"footnotes"},a.createElement(t.h2,{className:"sr-only",id:"footnote-label"},"Footnotes"),"\n",a.createElement(t.ol,null,"\n",a.createElement(t.li,{id:"user-content-fn-1"},"\n",a.createElement(t.p,null,"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. ",a.createElement(t.a,{href:"#user-content-fnref-1","data-footnote-backref":!0,className:"data-footnote-backref","aria-label":"Back to content"},"↩")," ",a.createElement(t.a,{href:"#user-content-fnref-1-2","data-footnote-backref":!0,className:"data-footnote-backref","aria-label":"Back to content"},"↩",a.createElement(t.sup,null,"2"))),"\n"),"\n",a.createElement(t.li,{id:"user-content-fn-2"},"\n",a.createElement(t.p,null,a.createElement(t.a,{href:"https://www.youtube.com/watch?v=mmzRYGCfTzc"},"https://www.youtube.com/watch?v=mmzRYGCfTzc")," ",a.createElement(t.a,{href:"#user-content-fnref-2","data-footnote-backref":!0,className:"data-footnote-backref","aria-label":"Back to content"},"↩")),"\n"),"\n",a.createElement(t.li,{id:"user-content-fn-3"},"\n",a.createElement(t.p,null,a.createElement(t.a,{href:"https://www.youtube.com/watch?v=WyuZvGt7RY4&t=2207s"},"https://www.youtube.com/watch?v=WyuZvGt7RY4&t=2207s")," ",a.createElement(t.a,{href:"#user-content-fnref-3","data-footnote-backref":!0,className:"data-footnote-backref","aria-label":"Back to content"},"↩")),"\n"),"\n",a.createElement(t.li,{id:"user-content-fn-4"},"\n",a.createElement(t.p,null,a.createElement(t.a,{href:"https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention"},"https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention")," ",a.createElement(t.a,{href:"#user-content-fnref-4","data-footnote-backref":!0,className:"data-footnote-backref","aria-label":"Back to content"},"↩")),"\n"),"\n"),"\n"))}var i=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,o.RP)(),e.components);return t?a.createElement(t,e,a.createElement(r,e)):r(e)},l=n(1173);function s(e){return a.createElement(l.A,e,a.createElement(i,e))}l.A}}]);
//# sourceMappingURL=component---node-modules-lekoarts-gatsby-theme-minimal-blog-core-src-templates-post-query-tsx-content-file-path-content-posts-attention-index-mdx-eb8400b49b4b88e0d573.js.map